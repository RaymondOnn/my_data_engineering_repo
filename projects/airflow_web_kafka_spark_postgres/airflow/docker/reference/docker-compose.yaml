version: "3"

services:
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - 5432:5432
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    volumes:
      - postgres:/var/lib/postgresql/data

  airflow-initdb:
    build: ..
    container_name: airflow-initdb
    depends_on:
      - postgres
    environment: &airflow-common-env
      AIRFLOW_HOME: /root/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    command: bash -c "airflow db migrate && airflow users create --firstname admin --lastname admin --email admin --password admin --username admin --role Admin"

#  webserver:
#    build: .
#    depends_on:
#      - initdb
#    environment:
#      <<: *airflow-common-env
#      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
#    healthcheck:
#      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
#      interval: 30s
#      timeout: 10s
#      retries: 5
#      start_period: 30s
#    restart: always
#    volumes:
#      - ./dags:/root/airflow/dags
#      - ./plugins:/root/airflow/plugins
#      - airflow-worker-logs:/root/airflow/logs
#    ports:
#      - "8080:8080"
#    command: airflow webserver

#  scheduler:
#    build: .
#    depends_on:
#      - webserver
#    environment:
#      <<: *airflow-common-env
#    healthcheck:
#      test: [ "CMD", "curl", "--fail", "http://localhost:8974/health" ]
#      interval: 30s
#      timeout: 10s
#      retries: 5
#      start_period: 30s
#      restart: always
#    volumes:
#      - ./dags:/root/airflow/dags
#      - ./plugins:/root/airflow/plugins
#      - airflow-worker-logs:/root/airflow/logs
#    command: airflow scheduler


  # --------------------- For adding celery workers --------------------------
#  airflow-redis:
#    image: redis:latest
#    expose:
#      - 6379
#    healthcheck:
#      test: [ "CMD", "redis-cli", "ping" ]
#      interval: 10s
#      timeout: 30s
#      retries: 50
#      start_period: 30s
#    restart: always
#
#  airflow-worker:
#    build: .
#    depends_on:
#      airflow-redis:
#        condition: service_healthy
#      airflow-postgres:
#        condition: service_healthy
#      airflow-init:
#        condition: service_completed_successfully
#    healthcheck:
#      test:
#        - "CMD-SHELL"
#        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
#      interval: 30s
#      timeout: 10s
#      retries: 5
#      start_period: 30s
#    environment:
#      <<: *airflow-common-env
#      # Required to handle warm shutdown of the celery workers properly
#      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
#      DUMB_INIT_SETSID: "0"
#    restart: always
#    command: celery worker
#
#  # You can enable flower by use: docker-compose --profile flower up
#  # or by explicitly targeted on the command line e.g. docker-compose up flower.
#  # See: https://docs.docker.com/compose/profiles/
#  airflow-flower:
#    build: .
#    depends_on:
#      airflow-redis:
#        condition: service_healthy
#      airflow-postgres:
#        condition: service_healthy
#      airflow-initdb:
#        condition: service_completed_successfully
#    profiles:
#      - flower
#    ports:
#      - "5555:5555"
#    environment:
#      <<: *airflow-common-env
#    healthcheck:
#      test: [ "CMD", "curl", "--fail", "http://localhost:5555/" ]
#      interval: 30s
#      timeout: 10s
#      retries: 5
#      start_period: 30s
#    restart: always
#    command: celery flower

volumes:
  postgres: {}
  airflow-worker-logs:
